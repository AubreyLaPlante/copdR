---
title: "howto_copdr"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{howto_copdr}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  markdown: 
    wrap: 72
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)

ThemeMain<-ggplot2::theme(legend.position = "none", plot.margin = grid::unit(c(0,0,0, 0), "npc"), 
                 panel.margin = grid::unit(c(0,0, 0, 0), "npc"), 
                 title = ggplot2::element_text(size=8, face='bold'),
                 #axis.text.y = element_blank(), 
                 axis.text.x = ggplot2::element_text(color='black'),
                 #axis.ticks.y = element_blank(),
                 axis.title.x = ggplot2::element_text(size=7,color='black',face='bold')
                 )+ggplot2::theme_minimal()
```

```{r setup}
library(copdr)
```



------------------------------------------------------------------------

# 1. Introduction to Cumulative-Offset Probability Distribution functions in R (copdr)

This workflow is meant to import probability distribution functions (PDFs) that were generated through the Matlab code LaDiCaoz. LaDiCaoz, a Matlab program that allows you to 'backslip' offset on a fault from digital elevation models (DEMs) or digital surface models (DSMs). It is a lot of acronyms, but stay with me. Each PDF is a normal distribution, or Gaussian distribution 
that represents one offset measurement on a fault. The first part of this workflow works to import and tidy up these data so that the user can visualize that all their data is plotting correctly on uniform axes. The second part of this workflow has a series of filters that will take the observations/metadata associated with each PDF (such as unit age, type of offset, standard deviation, uncertainty, etc) and sum the PDFs together. The goal of these filters is to remove the noisy PDFs or PDFs with high uncertainties (i.e. large standard deviations, high coefficient of variation, etc) so that clear peaks and trends in the data can be identified.

## 1.1 Load in data

First, you need to make sure that the full PDF dataset and its metadata
CSV is in a folder in your working directory. The default dataset is
pre-coded for your convenience.

### a. *User-defined inputs*

There are three global variables that the user can set:

1.  directory - this is where all your data are located.\
2.  folder_name - this is the folder_name in the directory that holds
    all the PDF .mat files.\
3.  csv_name - this is the reference metadata that has latitude,
    longitude, total, lateral, or vertical offset values, standard
    deviation of offset measurements, confidence rankings, and any other
    data that you collected. You may see example.csv or example.xlsx to
    see how this metadata should be formatted. However, this data must
    be in .csv form before loading it in.

```{r user-set-sources, results = 'hide'}
directory <<- system.file("extdata", package = "copdr")
folder_name <<- "example_mat"
csv_name <<- "example.csv"
```

### b. *Importing the data*

This first part of the code does the following:

1.  file2vect() takes the folder_name set above (i.e. 'example_mat') in
    the working directory and makes a vector of all the file names with
    the file format '.mat'.\
2.  mat2list() creates a list with three matrices, 1. lateral, 2.
    vertical, 3. total offset.
3.  Imports the metadata from a reference .csv file. See "example.csv"
    or "example.xlsx" in the external data section for how this
    reference metadata should be formatted.

```{r import-data, results = 'hide', message=FALSE}
backslip_names <- file2vect(directory, folder_name)

backslip_data <- mat2list(directory,folder_name,backslip_names)

backslip_csv <- readr::read_csv(file.path(directory,csv_name), show_col_types = FALSE)
```

------------------------------------------------------------------------

# 2. Data pre-processing

## 2.1. Visualize the raw data

This code plots all the PDFs data on a default coordinate axis, to check
that the data loaded in correctly. This is NOT the COPD plot yet, as we
have not summed the PDFs together, so the Y-axis is still probability
with a max of 1. With the default dataset, the axes are larger than the
extent of the data. You should not see any negative probabilities where the data plot in the negative Y domain.
For the lateral PDFs, negative values correspond to right-lateral motion and positive values correspond to left-lateral motion.
For the vertical PDFs, negative values correspond to extensional motion (down-dropping) and positive values correspond to compressional motion (thrusting). In the total offset PDF plot, these signs are ignored to calculate a total magnitude of displacement for each offset measurement.

```{r plot-raw, message=FALSE}
plotPDF(backslip_data[[1]],"lateral", simplify = "Y") + ThemeMain
plotPDF(backslip_data[[2]],"vertical",simplify = "Y") + ThemeMain
plotPDF(backslip_data[[3]],"total", simplify = "Y") + ThemeMain
```

## 2.2 Clip raw data and plot

Sometimes we may have datasets that are larger than the extent that we
would like to analyze. For example, in some cases we may only want to
look at small values of displacement, so we would need to clip out any
data that is larger than a threshold value. With the default dataset,
there are extra zeros on either side of each PDF curve, beyond the
maximum and minimum values. These extra zeros are unnecessary, and may
make it difficult when combining the datasets if each PDF has a different
length probability vector. So, we will uniformly clip the PDFs to
remove anything unnecessary for future analysis.

### a. *User-defined inputs*

Use the graphs from 1.1 to clip your dataset.

```{r user-clip-boundaries}
lat_limits <- list(
     lat_x_min = -5,
     lat_x_max = 0,
     lat_y_min = 0,
     lat_y_max = 1
)

vert_limits <- list(
     vert_x_min = -1,
     vert_x_max = 0,
     vert_y_min = 0,
     vert_y_max = 1
)

total_limits <- list(
     total_x_min = 0,
     total_x_max = 5,
     total_y_min = 0,
     total_y_max = 1
)  
```

### b. *Clipping the data*

The following code clips each of the datasets (lateral, vertical, total)
by the boundaries defined above, and plots the new, clipped datasets.

```{r clip-functions, message=FALSE, warning=FALSE}
#clip x limits of data
backslip_data_clip <- list(
     backslip_lat = data.clip(backslip_data[[1]],"lateral",lat_limits[[1]],lat_limits[[2]]),
     backslip_vert = data.clip(backslip_data[[2]],"vertical",vert_limits[[1]],vert_limits[[2]]),
     backslip_tot = data.clip(backslip_data[[3]],"total",total_limits[[1]],total_limits[[2]])
)

#plot clipped data
plotPDF(backslip_data_clip[[1]],"lateral", 
        lat_limits[[1]],lat_limits[[2]],lat_limits[[3]],lat_limits[[4]], simplify = "Y")+ ThemeMain
plotPDF(backslip_data_clip[[2]],"vertical", 
        vert_limits[[1]],vert_limits[[2]], vert_limits[[3]],vert_limits[[4]], simplify = "Y")+ ThemeMain
plotPDF(backslip_data_clip[[3]],"total", 
        total_limits[[1]],total_limits[[2]],total_limits[[3]],total_limits[[4]], simplify = "Y")+ ThemeMain
           
```

## 2.3 Confidence Scaling

This code scales each PDF by a confidence value. Confidence scales vary widely,
but are generally a study-specific ranking involving many qualitative
evaluations of how likely an offset geomorphic feature is representative
of an accurate and precise earthquake displacement value. This may
involve a relative ranking of how degraded a geomorphic feature is, the
angle that it intersects a fault trace (higher angles are better), or
whether there appears to be post-earthquake modification by streams or
human activity. These confidence values are specific to the dataset, where 1 is the maximum confidence and your
largest value is the lowest confidence. 

In this dataset, 3 is the lowest confidence. The code is set up to
accommodate any confidence scale (1 to 3, 1 to 5, 1 to 10, etc). If your
confidence values are flipped in the reference metadata, such that 1 is
the lowest confidence value, you will need to invert the values in your
metadata csv.

#### *Developer note to self:*

> Currently, this code is set up to perform confidence scaling on the
> clipped dataset from the previous section, and output a Cscaled
> dataset. In a future version of this code, each of these
> data-filtering and scaling sections will be optional, and be set up to
> perform confidence scaling on the backslip_data, replacing the input
> dataset with the output dataset from each section.

### a. *User-defined inputs*

Specify which column of the metadata .csv corresponds to the confidence
rankings, and the total number of offset geomorphic features. This
N_observations number should be equal to the number of .mat files in
your input folder_name. The default input dataset has 205 independent
offset measurements.

```{r user-set-confidence}
confidence_column = 9
N_observations = 205
```

### b. *Confidence-scaling the data*

```{r confidence-scaling, message=FALSE, warning=FALSE}
#scale the data by confidence values and put in a new Cscale list
backslip_data_Cscale <- backslip_data
for(i in 1:3){
     backslip_data_Cscale[[i]] <- conf.scale(backslip_data_clip[[i]],backslip_csv, confidence_column, N_observations)
}

#plot confidence-scaled data
plotPDF(backslip_data_Cscale[[1]],"lateral", 
             lat_limits[[1]],lat_limits[[2]],lat_limits[[3]],lat_limits[[4]], simplify = "Y")+ ThemeMain
plotPDF(backslip_data_Cscale[[2]],"vertical", 
        vert_limits[[1]],vert_limits[[2]], vert_limits[[3]],vert_limits[[4]], simplify = "Y")+ ThemeMain
plotPDF(backslip_data_Cscale[[3]],"total",
        total_limits[[1]],total_limits[[2]],total_limits[[3]],total_limits[[4]], simplify = "Y")+ ThemeMain
```

------------------------------------------------------------------------

## 2.4 Coefficient of variation (COV) calculation

The coefficient of variation (COV) is the ratio of the standard
deviation ($\sigma$) to the sample mean ($\mu$) for a single PDF and is
a statistical measure of the relative dispersion (also called scatter,
or spread) of the dataset.

$$COV = \frac{\sigma}{\mu}$$

This parameter is an alternative to standard deviation to measure the
relative uncertainty of offset geomorphic features. One disadvantage of
filtering by COV is that if standard deviations are relatively constant
across a dataset, distributions with smaller means will tend to have
larger COVs. Therefore, removing PDFs with larger COVs may lessen, or
remove entirely, peaks at displacement means \< 1 m. Generally, standard
deviation tends to be a more reliable measure of relative uncertainty
for smaller magnitude offsets, while coefficient of variation tends to
be a more relable measure of relative uncertainty for larger magnitude
offsets. In any case, it is useful to have this measurement of
dispersion in your reference csv.

The master reference CSV "backslip_csv" should have 4-6 columns for
displacement statistics including:\
- lateral mean\
- lateral standard deviation\
- vertical mean\
- vertical standard deviation\
- total mean\
- total standard devation\

The inputs for this section will need to be the columns of offset data.
The following code calculates the coefficient of variation, which is the
standard deviation divided by the mean. I used lateral in my example
because in the example data, vertical displacement is poorly resolved
and of very small magnitude.

### *User-defined inputs*

```{r user-set-datacolumns}
# these are the column numbers corresponding to this data in the reference csv
columnNums <- list(
     lat_mean = 13,
     lat_stdev = 14,
     vert_mean = 15,
     vert_stdev = 16,
     total_mean = 17,
     total_stdev = 18)

```

### *Calculate COV values*

```{r cov-calc}
backslip_csv <- calc.cov(backslip_csv,"lateral", columnNums[[1]], columnNums[[2]])
backslip_csv <- calc.cov(backslip_csv,"vertical",columnNums[[3]], columnNums[[4]])
backslip_csv <- calc.cov(backslip_csv,"total",columnNums[[5]], columnNums[[6]])
```

------------------------------------------------------------------------

# 3. Filters

The goal of filtering PDFs prior to summing them together is to find
where clusters of data produce the largest, most statistically
significant peaks. In selecting certain PDFs for COPD analysis, we want
to minimize the amount of dispersion and uncertainty that could obscure
the peaks associated earthquake magnitudes. However, we do not want to
remove enough data that we begin to introduce statistically
insignificant peaks, such that we over-interpret the number of
earthquakes. We will tackle the statistical significance part with an
optimization problem later on in the code.

As discussed above in section 2.4, there are advantages and
disadvantages to filtering data through a large COV filter. There are
also advantages and disadvantages to using only standard deviation
filters. One problem with removing PDFs with larger standard deviations
is that this filtering method will produce a bias towards removing
larger displacements in older fan surfaces, due to the time-degradation
of that geomorphic surface and an increase in uncertainty of the offset.

Ultimately, it is your job to pick and choose the filters that produce
the most statistically significant peaks. This section contains *(it
will eventually contain)* filters for standard deviation, coefficient of
variation, geomorphic-age, and piercing point type. Each of these
filters will work via the following steps:

-User-defined set of criteria for a single category/column of the
reference metadata .csv (geomorphic feature, standard deviation, etc)
-Create a clipped metadata .csv with only the values/characteristics
indicated by the filter (that meet a set of critera) -Clip the input
offset matrix to include only filenames in the clipped metadata -Sum
these PDFs together to create a COPD

## 4.1 COV filters

### a. User-defined inputs

Specify the threshold values of COV that you want to remove from the
input matrices.

```{r user-set-cov-filter}
#these are the COV values you want to filter for
COV_filter_vals <- seq(.10,.30, by=0.05) 
```

### b. Filter PDF data by threshold COV values

The following code creates the clipped reference .csv(s) for each of the
threshold values set above, and then filters the input matrix (**note:
*currently, the Cscaled dataset*** ) using the clipped .csv metadata.

```{r filter-threshold-cov, echo = FALSE, message = FALSE, warning = FALSE}
# this creates reference data files for the offset_IDs and the corresponding COV values
lat_covREF <- filter_covREF(backslip_csv,1,"lateral",COV_filter_vals)
vert_covREF <- filter_covREF(backslip_csv,1,"vertical",COV_filter_vals)
total_covREF <- filter_covREF(backslip_csv,1,"total",COV_filter_vals)

### this filters the input datatable 'backslip_data' by the filtered COV reference files created using filter.covREF
lat_cov2 <- filter_covDAT(backslip_csv,1,"lateral",COV_filter_vals,lat_covREF,backslip_data_Cscale)
vert_cov2 <- filter_covDAT(backslip_csv,1,"vertical",COV_filter_vals,vert_covREF,backslip_data_Cscale)
total_cov2 <- filter_covDAT(backslip_csv,1,"total",COV_filter_vals,total_covREF,backslip_data_Cscale)

```

The code then produces the COPD plots.

### *Developer's note*

> This section is very code-heavy and I am in progress of turning this
> into a function. ideally, the user will be able to plot every
> COV-threshold filtered copd on the same figure, so that they can
> compare which filter provides the best results to analyze peaks.

```{r plot-cov-filter}
latPlot <- vertPlot <- totPlot <- list()
for (i in 1:length(COV_filter_vals)){
     latPlot[[i]] <- plotCOPD(lat_cov2[[i]],"lateral",COV_filter_vals[i], lat_limits[[1]], lat_limits[[2]]) + ThemeMain
}
for (i in 1:length(COV_filter_vals)){
     vertPlot[[i]] <- plotCOPD(vert_cov2[[i]],"vertical",COV_filter_vals[i], vert_limits[[1]],vert_limits[[2]]) + ThemeMain
}

for (i in 1:length(COV_filter_vals)){
     totPlot[[i]] <- plotCOPD(total_cov2[[i]],"total",COV_filter_vals[i], total_limits[[1]],total_limits[[2]]) + ThemeMain
}

for(i in 1:length(COV_filter_vals)){
     gridExtra::grid.arrange(latPlot[[i]])
}

for(i in 1:length(COV_filter_vals)){
     gridExtra::grid.arrange(vertPlot[[1]])
}

for(i in 1:length(COV_filter_vals)){
     gridExtra::grid.arrange(totPlot[[i]])
}

#plotCOPD(lat_cov2[[5]] ,"lateral", lat_limits[[1]], lat_limits[[2]])
#plotCOPD(vert_cov2[[5]] ,"vertical", vert_limits[[1]],vert_limits[[2]])
#plotCOPD(total_cov2[[5]] ,"total", total_limits[[1]],total_limits[[2]])
```

Which COV filter produces the best peaks, without removing too much of
the original data?

### *Developer's note*

> I want to add an 'n = \_\_ ' on the graphs, so the user can see how
> many total PDF observations are summed together for each COPD.
